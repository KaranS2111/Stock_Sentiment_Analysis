{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import requests\n",
        "import csv"
      ],
      "metadata": {
        "id": "vU__I7NDyuF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "import nltk\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from scipy.special import softmax\n",
        "import time\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "from textblob import TextBlob\n",
        "import re\n",
        "from collections import defaultdict\n",
        "# from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, precision_score\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "import yfinance as yf\n",
        "import os\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.metrics import roc_curve, roc_auc_score, auc\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncaaFgUr0W5j",
        "outputId": "ca0c7ecd-3e85-4609-fd1b-93a0450724f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text, re.I|re.A)\n",
        "\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    def get_wordnet_pos(word):\n",
        "        tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "        tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
        "        return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token, get_wordnet_pos(token)) for token in stemmed_tokens]\n",
        "\n",
        "    return \" \".join(lemmatized_tokens)"
      ],
      "metadata": {
        "id": "oL7OrMJfzPhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tl8gNn9_yoYD",
        "outputId": "84d20ae8-3030-4bef-e208-9cf87eb4a905"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://api.nytimes.com/svc/archive/v1/2024/1.json?api-key=KPdZEAPGHoazqGaFckkGm1dNlCxULV85\n",
            "<Response [200]>\n",
            "Status code: 200\n",
            "https://api.nytimes.com/svc/archive/v1/2024/2.json?api-key=KPdZEAPGHoazqGaFckkGm1dNlCxULV85\n",
            "<Response [200]>\n",
            "Status code: 200\n",
            "https://api.nytimes.com/svc/archive/v1/2024/3.json?api-key=KPdZEAPGHoazqGaFckkGm1dNlCxULV85\n",
            "<Response [200]>\n",
            "Status code: 200\n",
            "https://api.nytimes.com/svc/archive/v1/2024/4.json?api-key=KPdZEAPGHoazqGaFckkGm1dNlCxULV85\n",
            "<Response [200]>\n",
            "Status code: 200\n",
            "https://api.nytimes.com/svc/archive/v1/2024/5.json?api-key=KPdZEAPGHoazqGaFckkGm1dNlCxULV85\n",
            "<Response [200]>\n",
            "Status code: 200\n"
          ]
        }
      ],
      "source": [
        "api_key = 'GxvFJaFLlG9NOdncyCQUQkXxaRe7nvA5'\n",
        "\n",
        "sections_to_include = [\n",
        "    \"Corrections\", \"New York\", \"International Home\", \"U.S.\", \"Business Day\",\n",
        "    \"Real Estate\", \"World\", \"Times Insider\", \"T Magazine\",\n",
        "    \"Technology\", \"Magazine\", \"Your Money\", \"Obituaries\"\n",
        "]\n",
        "\n",
        "articles_by_date = defaultdict(list)\n",
        "for year in range(2024, 2025):\n",
        "\n",
        "    for month in range(1, 6):\n",
        "        url = f\"https://api.nytimes.com/svc/archive/v1/{year}/{month}.json?api-key={api_key}\"\n",
        "        print(url)\n",
        "        response = requests.get(url)\n",
        "        print(response)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            print(f\"Status code: {response.status_code}\")\n",
        "\n",
        "            for article in data['response']['docs']:\n",
        "                section_name = article['section_name']\n",
        "                if section_name in sections_to_include:\n",
        "                    date = article['pub_date'][:10]\n",
        "                    headline = article['headline']['main']\n",
        "                    articles_by_date[date].append(headline)\n",
        "\n",
        "        else:\n",
        "            print(f\"Failed API. Status code: {response.status_code}\")\n",
        "\n",
        "with open(f'nyt_articles_2024.csv', mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(['Date', 'Headlines'])\n",
        "\n",
        "    for date, headlines in articles_by_date.items():\n",
        "        joined_headlines = \" \".join(headlines)\n",
        "        writer.writerow([date, preprocess_text(joined_headlines)])\n",
        "\n",
        "# In the main code I have not used this multiple times and only used this once to get the simple file.\n",
        "# The file is nyt_articles_io.csv which came using this code."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xz9WwdjOyqfv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}